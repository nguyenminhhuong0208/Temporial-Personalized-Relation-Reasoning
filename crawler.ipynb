{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dữ liệu FireAnt\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hằng số chung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dãy Authenication Bearer Token cần sử dụng để gọi API\n",
    "AUTH_BEARER = ''\n",
    "\n",
    "# Dãy các key giá trị cần lấy từ API\n",
    "USEFUL_KEYS = ['postID', 'date', 'postGroup', 'title', 'description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hàm gọi API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def get_request(url, params=None):\n",
    "    \"\"\"\n",
    "    Hàm gọi API chung tới FireAnt\n",
    "    Trả về dữ liệu JSON nếu thành công, None nếu thất bại\n",
    "\n",
    "    url: đường dẫn API, params: tham số gửi đi\n",
    "    \"\"\"\n",
    "\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {AUTH_BEARER}',\n",
    "    }\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f'u failed god: {response.status_code}')\n",
    "        return None\n",
    "    return response.json()\n",
    "\n",
    "def get_posts(offset = 0, limit = 10):\n",
    "    \"\"\"\n",
    "    Hàm lấy danh sách các bài viết\n",
    "\n",
    "    offset: vị trí bắt đầu lấy, limit: số lượng bài viết cần lấy\n",
    "    \"\"\"\n",
    "\n",
    "    url = 'https://api.fireant.vn/posts'\n",
    "    params = {\n",
    "        'type': 1,\n",
    "        'offset': offset,\n",
    "        'limit': limit,\n",
    "    }\n",
    "    return get_request(url, params)\n",
    "\n",
    "def get_replies(post_id, offset = 0, limit = 10):\n",
    "    \"\"\"\n",
    "    Hàm lấy danh sách các bình luận của một bài viết\n",
    "\n",
    "    post_id: id của bài viết cần lấy, offset: vị trí bắt đầu lấy, limit: số lượng bình luận cần lấy\n",
    "    \"\"\"\n",
    "\n",
    "    url = f'https://api.fireant.vn/posts/{post_id}/replies'\n",
    "    params = {\n",
    "        'offset': offset,\n",
    "        'limit': limit,\n",
    "    }\n",
    "    return get_request(url, params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hàm xử lý dữ liệu bài viết\n",
    "\n",
    "Hàm này là hàm helper, đầu vào là một bài viết đã được cào, để xử lý các key và bỏ các key không cần thiết.\n",
    "Trả về bài viết đã được xử lý"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_post(post: dict):\n",
    "    \"\"\"\n",
    "    Hàm xử lý dữ liệu bài viết, xử lý qua các key và bỏ các key không cần thiết\n",
    "    Trả về bài viết đã được xử lý\n",
    "    \"\"\"\n",
    "\n",
    "    # Chỉ giữ lại các key cần thiết\n",
    "    post = {k: v for k, v in post.items() if k in USEFUL_KEYS}\n",
    "    \n",
    "    # # Xử lý các mã được đề cập trong key 'taggedSymbols'\n",
    "    # list_symbol = post['taggedSymbols']\n",
    "    # list_processed_symbol = [] # Danh sách các mã sau khi xử lý\n",
    "\n",
    "    # for symbol in list_symbol:\n",
    "    #     # symb: mã cổ phiếu\n",
    "    #     # price: giá cổ phiếu tại thời điểm bài viết được đăng\n",
    "    #     try:\n",
    "    #         list_processed_symbol.append({\n",
    "    #             'symb': symbol['symbol'],\n",
    "    #             'price': round(float(symbol['price']),2),\n",
    "    #         })\n",
    "    #     except:\n",
    "    #         pass\n",
    "    \n",
    "    # post['taggedSymbols'] = json.dumps(list_processed_symbol) # Gán lại giá trị mới cho key 'taggedSymbols'\n",
    "\n",
    "    # Xử lý các key cần thiết\n",
    "    # post['userid'] = post['user']['id']\n",
    "    # post['totalImages'] = len(post['images'])\n",
    "    # post['totalFiles'] = len(post['files'])\n",
    "    # post['totalSymbols'] = len(list_processed_symbol)\n",
    "    \n",
    "    # Xóa các key không cần thiết\n",
    "    # del post['user']\n",
    "    # del post['images']\n",
    "\n",
    "    try:\n",
    "        post['group'] = str(post['postGroup']['name'])\n",
    "    except:\n",
    "        post['group'] = \"Không xác định\"\n",
    "    \n",
    "    try:\n",
    "        del post['postGroup']\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hàm cào bài viết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateutil.parser\n",
    "from datetime import datetime\n",
    "import csv\n",
    "\n",
    "def crawl_posts(offset = 0, limit = 1000, number_of_entries = 26262, end_date_str = '2024-12-29T00:00:00+07:00'):\n",
    "    \"\"\"\n",
    "    Hàm cào dữ liệu bài viết từ FireAnt\n",
    "\n",
    "    offset: vị trí bắt đầu, limit: số lượng bài viết lấy một lúc, number_of_entries: số lượng bài viết tối đa cần lấy\n",
    "    end_date_str: ngày kết thúc lấy dữ liệu\n",
    "    \"\"\"\n",
    "\n",
    "    result_posts_id = {}        # Dict tổng hợp ID các bài viết đã lấy, tránh trùng lặp\n",
    "    count_posts = 0             # Số lượng bài viết đã lấy\n",
    "    count_api_call = 0          # Số lần gọi API\n",
    "\n",
    "    # Lặp cho đến khi lấy đủ số lượng bài viết cần lấy, hoặc hết dữ liệu\n",
    "    while count_posts < number_of_entries:\n",
    "        count_api_call += 1\n",
    "        data = get_posts(offset, limit) # Lấy dữ liệu từ API\n",
    "\n",
    "        if not data: # Nếu dữ liệu rỗng, thoát vòng lặp\n",
    "            break\n",
    "        \n",
    "        # Nếu thời gian của bài viết cuối cùng nhỏ hơn ngày end_date, thoát vòng lặp\n",
    "        # Ta chỉ lấy dữ liệu đến ngày end_date\n",
    "        cur_date = dateutil.parser.isoparse(data[-1]['date']) # Thời gian của bài viết cuối cùng\n",
    "        end_date = dateutil.parser.isoparse(end_date_str) # Thời gian kết thúc lấy dữ liệu\n",
    "        if cur_date < end_date:\n",
    "            break\n",
    "        \n",
    "        count_overlap = 0 # Số lượng bài viết trùng lặp\n",
    "        result_posts = [] # Danh sách bài viết cần lấy\n",
    "\n",
    "        for post in data:\n",
    "            # Nếu bài viết đã được lấy, tăng biến đếm và bỏ qua\n",
    "            if post['postID'] in result_posts_id: \n",
    "                count_overlap += 1\n",
    "                continue\n",
    "            \n",
    "            # Thêm ID bài viết vào danh sách đã lấy\n",
    "            result_posts_id[post['postID']] = True \n",
    "            result_posts.append(process_post(post)) \n",
    "            count_posts += 1\n",
    "        \n",
    "        # Tính lại offset cho lần lấy tiếp theo\n",
    "        offset += limit + count_overlap\n",
    "\n",
    "        # Ghi dữ liệu vào file CSV\n",
    "        keys = result_posts[0].keys() # Lấy danh sách key của dữ liệu\n",
    "        with open('posts.csv', 'a', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=keys) # Tạo writer\n",
    "            if(count_api_call == 1): # Nếu là lần ghi đầu tiên, ghi header\n",
    "                writer.writeheader()\n",
    "            writer.writerows(result_posts) # Ghi dữ liệu vào file\n",
    "\n",
    "        print(f'Call no. {count_api_call}, this time got: {len(result_posts)} posts, total crawled {count_posts} posts, offset: {offset}', end='\\r')\n",
    "\n",
    "    print(f'\\nCrawled {count_posts} posts, end of data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hàm cào bình luận\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from time import sleep as delay\n",
    "\n",
    "def crawl_replies(offset = 0, limit = 1000, skip = 0, range_of_replies = (20, 1000)):\n",
    "    \"\"\"\n",
    "    Hàm cào dữ liệu bình luận từ FireAnt, sử dụng danh sách bài viết `posts.csv`.\n",
    "\n",
    "    offset: vị trí bắt đầu (bình luận), limit: số lượng bình luận lấy một lúc\n",
    "    skip: số lượng bài viết ban đầu được bỏ qua\n",
    "    range_of_replies: bài có số lượng bình luận nằm trong khoảng này mới được lấy\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv('posts.csv')   # Dataframe chính từ danh sách bài viết trong file CSV\n",
    "    count_replies = 0               # Số lượng bình luận đã lấy\n",
    "\n",
    "    # Lọc các bài viết có số lượng bình luận nằm trong khoảng range_of_replies\n",
    "    post_list = df[(df['totalReplies'] >= range_of_replies[0]) & (df['totalReplies'] <= range_of_replies[1])]['postID'].tolist()\n",
    "    print(f'Found {len(post_list)} posts with totalReplies between {range_of_replies[0]} and {range_of_replies[1]}')\n",
    "\n",
    "    # Xét từng bài viết\n",
    "    for post_idx, postID in enumerate(post_list):\n",
    "        if post_idx < skip: # Bỏ qua số lượng bài viết ban đầu\n",
    "            continue\n",
    "\n",
    "        # Thử lấy dữ liệu bình luận cho bài viết, tối đa 4 lần\n",
    "        data = get_replies(postID, offset, limit)\n",
    "        if not data: \n",
    "            # Nếu không lấy được dữ liệu, thử lại 3 lần, mỗi lần delay 3 giây\n",
    "            for retry in range(3):\n",
    "                print(f'Failed to get replies for post no. {postID} ({post_idx+1}/{len(post_list)}), retrying... ({retry+1}/3)', end='\\r')\n",
    "                data = get_replies(postID, offset, limit)\n",
    "                if data: # Nếu lấy được dữ liệu, thoát vòng lặp\n",
    "                    break \n",
    "                delay(3)\n",
    "\n",
    "        # Nếu vẫn không lấy được dữ liệu, bỏ qua bài viết\n",
    "        if not data:\n",
    "            print(f'Failed to get replies for post no. {postID} ({post_idx+1}/{len(post_list)}), skipping...')\n",
    "            continue\n",
    "\n",
    "        result_replies = [] # Danh sách bình luận cần lấy\n",
    "\n",
    "        # Xử lý dữ liệu bình luận\n",
    "        for post in data:\n",
    "            result_replies.append(process_post(post))\n",
    "            count_replies += 1\n",
    "\n",
    "        # Ghi dữ liệu vào file CSV\n",
    "        keys = result_replies[0].keys() # Lấy danh sách key của dữ liệu\n",
    "        with open('replies.csv', 'a', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=keys) # Tạo writer\n",
    "            if(post_idx == 0): # Nếu là lần ghi đầu tiên, ghi header\n",
    "                writer.writeheader()\n",
    "            writer.writerows(result_replies) # Ghi dữ liệu vào file\n",
    "        \n",
    "        print(f'Crawled post no. {postID} ({post_idx+1}/{len(post_list)}), got {len(result_replies)} replies, total crawled {count_replies} replies', end='\\r')\n",
    "        delay(0.2) # Delay 0.2 giây giữa các bài viết, tránh bị block\n",
    "    \n",
    "    print(f'\\nCrawled {len(post_list)} posts, end of data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hàm làm sạch dữ liệu\n",
    "Hàm này để làm sạch dữ liệu trong các file CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def clean_data(csvfile):\n",
    "    \"\"\"\n",
    "    Hàm xử lý dữ liệu sau khi cào, xóa các bài viết trùng lặp và các cột không cần thiết\n",
    "\n",
    "    csvfile: file CSV cần xử lý\n",
    "    \"\"\"\n",
    "\n",
    "    # Đọc file CSV\n",
    "    df = pd.read_csv(csvfile) \n",
    "\n",
    "    # Xóa các bài viết trùng lặp\n",
    "    df = df.drop_duplicates(subset=['postID'])\n",
    "\n",
    "    # Ghi dữ liệu vào file CSV mới (cleaned_{csvfile})\n",
    "    df.to_csv(f'cleaned_{csvfile}', index=False)\n",
    "\n",
    "    print(f'Cleaned data saved to cleaned_{csvfile}, total {len(df)} entries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Thực thi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call no. 24, this time got: 1000 posts, total crawled 24000 posts, offset: 24000\n",
      "Crawled 24000 posts, end of data\n"
     ]
    }
   ],
   "source": [
    "# Cào dữ liệu bài viết\n",
    "crawl_posts(limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to cleaned_posts.csv, total 24000 entries\n"
     ]
    }
   ],
   "source": [
    "# Làm sạch dữ liệu trong 2 file CSV\n",
    "clean_data('posts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18983\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('cleaned_posts.csv')\n",
    "df = df[df['group'] != \"Doanh nghiệp\"]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
